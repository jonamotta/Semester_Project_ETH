{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.16/00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jonamotta/anaconda3/envs/python_root/lib/python2.7/site-packages/root_numpy/__init__.py:46: RuntimeWarning: numpy 1.16.3 is currently installed but you installed root_numpy against numpy 1.9.3. Please consider reinstalling root_numpy for this numpy version.\n",
      "  RuntimeWarning)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import ROOT\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import root_pandas\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import uproot\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "#from root_numpy import root2array\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "### READ TREES AND CREATE DATAFRAMES ###\n",
    "########################################\n",
    "\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(1986)\n",
    "\n",
    "features = [\n",
    "    'mu_pt'     ,\n",
    "    'mu_eta'    ,\n",
    "    'mu_phi'    ,\n",
    "    'mu_charge' ,\n",
    "    'mu1_pt'    ,\n",
    "    'mu1_eta'   ,\n",
    "    'mu1_phi'   ,\n",
    "    'mu1_charge',\n",
    "    'mu2_pt'    ,\n",
    "    'mu2_eta'   ,\n",
    "    'mu2_phi'   ,\n",
    "    'mu2_charge',\n",
    "]\n",
    "\n",
    "#create DataFrames with the values coming from the trees\n",
    "file_tau = uproot.open('../bc_jpsi_tau_nu_gen_v2.root')\n",
    "tree_tau = file_tau['tree;1']\n",
    "tau  = tree_tau.pandas.df(tree_tau.keys())\n",
    "\n",
    "file_mu = uproot.open('../bc_jpsi_mu_nu_gen_v2.root')\n",
    "tree_mu = file_mu['tree;2']\n",
    "mu  = tree_mu.pandas.df(tree_mu.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "### PREPARE DFs FOR TRAINING AND TEST ###\n",
    "#########################################\n",
    "\n",
    "\n",
    "#add the column target to both dataframes\n",
    "mu ['target'] = 0\n",
    "tau['target'] = 1\n",
    "\n",
    "# concatenate the two samples\n",
    "dataset = pd.concat([mu, tau])\n",
    "\n",
    "# shuffle and split train/test\n",
    "train, test = train_test_split(dataset, test_size=0.85, random_state=1986, shuffle=True)\n",
    "\n",
    "# X and Y on the training sample\n",
    "X = pd.DataFrame(train, columns=features)\n",
    "Y = pd.DataFrame(train, columns=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jonamotta/anaconda3/envs/python_root/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "########################\n",
    "### DEFINE THE MODEL ###\n",
    "########################\n",
    "\n",
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=len(features), activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sequential()**: \n",
    "\n",
    "    essentially stack one after the other the layers of the NN\n",
    "\n",
    "**Dense()**:\n",
    "\n",
    "    creates the single layer of the NN\n",
    "    Dense(int=output_array_shape [(*,int)],imput_dim [(*,input_dim)],activation_fct)\n",
    "\n",
    "An **activation function**: \n",
    "\n",
    "    It is a transfer function that is used to map the output of one layer to another\n",
    "    e.g. \n",
    "        relu:\n",
    "        A straight line function where activation is proportional to input (which is the weighted sum from neuron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An **error function** must be chosen in order to estimate if I am doing things right or wrong: for NN the error function is called **loss function**. It is used to modify the weights in order to improve the result at the following iteration of the net\n",
    "\n",
    "The **cross entropy** between two probability distributions p and q over the same underlying set of events measures the average number of bits needed to identify an event drawn from the set if a coding scheme used for the set is optimized for an estimated probability distribution q, rather than the true distribution p. **It's the default choice for binary problems**\n",
    "\n",
    "**adam** (**ada**ptive **m**oment estimation) is an optimization algorithm. When compiling a NN you must choose what type of optimization algorith you NN should follow\n",
    "\n",
    "**metric function** is similar to a loss function, except that the results from evaluating a metric are not used when training the model. You may use any of the loss functions as a metric function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jonamotta/anaconda3/envs/python_root/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 48422 samples, validate on 48422 samples\n",
      "Epoch 1/10\n",
      "48422/48422 [==============================] - 8s 170us/step - loss: 0.5662 - acc: 0.6971 - val_loss: 0.5612 - val_acc: 0.7055\n",
      "Epoch 2/10\n",
      "48422/48422 [==============================] - 8s 157us/step - loss: 0.5575 - acc: 0.7142 - val_loss: 0.5556 - val_acc: 0.7180\n",
      "Epoch 3/10\n",
      "48422/48422 [==============================] - 8s 157us/step - loss: 0.5504 - acc: 0.7224 - val_loss: 0.5472 - val_acc: 0.7272\n",
      "Epoch 4/10\n",
      "48422/48422 [==============================] - 8s 157us/step - loss: 0.5349 - acc: 0.7403 - val_loss: 0.5258 - val_acc: 0.7475\n",
      "Epoch 5/10\n",
      "48422/48422 [==============================] - 8s 157us/step - loss: 0.5093 - acc: 0.7639 - val_loss: 0.5001 - val_acc: 0.7640\n",
      "Epoch 6/10\n",
      "48422/48422 [==============================] - 8s 156us/step - loss: 0.4899 - acc: 0.7747 - val_loss: 0.4824 - val_acc: 0.7808\n",
      "Epoch 7/10\n",
      "48422/48422 [==============================] - 8s 163us/step - loss: 0.4805 - acc: 0.7780 - val_loss: 0.4752 - val_acc: 0.7810\n",
      "Epoch 8/10\n",
      "48422/48422 [==============================] - 8s 161us/step - loss: 0.4756 - acc: 0.7786 - val_loss: 0.4755 - val_acc: 0.7816\n",
      "Epoch 9/10\n",
      "48422/48422 [==============================] - 8s 159us/step - loss: 0.4736 - acc: 0.7813 - val_loss: 0.4680 - val_acc: 0.7845\n",
      "Epoch 10/10\n",
      "48422/48422 [==============================] - 7s 154us/step - loss: 0.4713 - acc: 0.7815 - val_loss: 0.4681 - val_acc: 0.7840\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1086de150>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-03 09:40:19.092135: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "### TRAIN THE MODEL ###\n",
    "#######################\n",
    "\n",
    "\n",
    "# fit the model\n",
    "model.fit(X, Y, epochs=10, batch_size=10, validation_split=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**fit** trains the model for a given number of epochs (iterations on a dataset)\n",
    "\n",
    "**batch_size** is the number of samples per gradient update (default is 32)\n",
    "\n",
    "**validation_split**: float between 0 and 1. Fraction of the training data to be used as validation data. The model will set apart this fraction of the training data, will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch. The validation data is selected from the last samples in the x and y data provided, before shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96844/96844 [==============================] - 1s 13us/step\n",
      "\n",
      "loss: 46.80%\n",
      "acc: 78.47%\n",
      "predicting on 548784 events\n"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "### TEST THE TRAINING OF THE MODEL ###\n",
    "######################################\n",
    "\n",
    "\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X, Y)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[0], scores[0]*100))\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "\n",
    "# calculate predictions on the test sample\n",
    "print 'predicting on', test.shape[0], 'events'\n",
    "x = pd.DataFrame(test, columns=features)\n",
    "y = model.predict(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**evaluate** returns the loss value & metrics values for the model in test mode\n",
    "\n",
    "**predict** generates output predictions for the input samples. Computation is done in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the score to the test sample dataframe\n",
    "test.insert(len(test.columns), 'nn_score', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file\n",
    "#test.to_root('bc_enriched.root', key='tree', store_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "### ROC CURVE ###\n",
    "#################\n",
    "\n",
    "\n",
    "# let sklearn do the heavy lifting and compute the ROC curves for you\n",
    "fpr, tpr, wps = roc_curve(test.target, test.nn_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ROC curve** (Receiver Operating Characteristic curve) is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings\n",
    "\n",
    "the function returns:\n",
    "- fpr : array, shape = [>2] _ Increasing false positive rates such that element i is the false positive rate of predictions with score >= thresholds[i]\n",
    "- tpr : array, shape = [>2] _ Increasing true positive rates such that element i is the true positive rate of predictions with score >= thresholds[i] \n",
    "- wps : array, shape = [n_thresholds] _ Decreasing thresholds on the decision function used to compute fpr and tpr. thresholds[0] represents no instances being predicted and is arbitrarily set to max(y_score) + 1+\n",
    "\n",
    "**AUC** (Area Under the Curve) tells us how much our model is capable of doing the correct discrimination:\n",
    "- area -> 1 very good correct discrimination (recognizes 0 as 0 and 1 as 1 almost always)\n",
    "- area -> 0 very good opposite discrimination (recognizes 0 as 1 and 1 as 0 almost always)\n",
    "- area -> 0.5 zero discrimination power, it's almost a random choosing of category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### ROC & SCORE CURVES ###\n",
    "##########################\n",
    "\n",
    "\n",
    "# plot\n",
    "plt.plot(fpr, tpr, label=r'$\\mu$ vs. $\\tau$ NN', color='b')\n",
    "# plot also the diagonal, that corresponds to random picks, no discrimination power\n",
    "xy = [i*j for i,j in product([10.**i for i in range(-8, 0)], [1,2,4,8])]+[1]\n",
    "plt.plot(xy, xy, color='grey', linestyle='--', label='no-discrimination curve')\n",
    "# cosmetics\n",
    "plt.title('ROC curve')\n",
    "plt.xlabel(r'$\\epsilon(\\tau)$')\n",
    "plt.ylabel(r'$\\epsilon(\\mu)$')\n",
    "# axis range\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "# grid\n",
    "plt.grid(True)\n",
    "# legend\n",
    "plt.legend(loc='best')\n",
    "# save figure and then clean it\n",
    "plt.savefig('graphs/nn_roc.pdf')\n",
    "plt.show()\n",
    "#plt.clf()\n",
    "\n",
    "# plot the discriminator shape for the muon and tau channel\n",
    "test_tau = test[test.tau_pt>=0]\n",
    "test_mu  = test[test.tau_pt <0]\n",
    "sb.distplot(test_tau['nn_score'], kde=False, norm_hist=True, label=r'$\\tau\\to\\mu$ channel', color='b')\n",
    "sb.distplot(test_mu ['nn_score'], kde=False, norm_hist=True, label=r'$\\mu$ channel' , color='r')\n",
    "plt.title('Classifier distribution')\n",
    "plt.xlabel('classifier score')\n",
    "plt.ylabel('a.u.')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.legend(loc='best')\n",
    "plt.savefig('graphs/nn_score.pdf')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the auc\n",
    "auroc = auc(fpr, tpr)\n",
    "print \"AUC =\", auroc \n",
    "\n",
    "# compute Gini index\n",
    "print \"Gini index =\", (auroc-0.5)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
